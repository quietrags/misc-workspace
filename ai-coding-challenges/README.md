# AI-Augmented Coding Challenge Framework

**Pushing the Limits of AI-Assisted Software Engineering**

---

## ğŸ¯ Project Overview

This project provides a comprehensive framework for evaluating and demonstrating the cutting-edge capabilities of AI-augmented coding tools, specifically designed to showcase the full spectrum of GitHub Copilot modes (Chat, Agentic, Workspace) and similar AI coding assistants.

### Purpose

- **Primary Goal:** Create challenging, realistic scenarios that truly benefit from AI augmentation
- **Target Audience:** Engineers who want to push AI coding tools to their limits
- **Challenge Format:** Choose 2 out of 3 challenges, complete within defined timeframe
- **Evaluation Focus:** Quality, architecture, AI utilization, and real-world applicability

### Key Principles

- âœ… **AI-Native Challenges:** Problems designed specifically where AI augmentation provides significant advantage
- âœ… **Mode Diversity:** Challenges that benefit from different AI modes (chat, agentic, workspace)
- âœ… **Real-World Relevance:** Scenarios based on actual enterprise engineering challenges
- âœ… **Measurable Outcomes:** Clear success criteria and evaluation rubrics
- âœ… **Skill Spectrum:** Mix of architecture, refactoring, security, and system design

---

## ğŸ“‹ Challenge Selection Philosophy

### What Makes a Great AI-Augmented Challenge?

The ideal AI coding challenge should:

1. **Require Complex Context Understanding**
   - Multi-file coordination
   - Cross-language integration
   - Legacy codebase navigation
   - Pattern recognition across large codebases

2. **Benefit from Different AI Modes**
   - **Chat Mode:** Quick queries, documentation lookup, API exploration
   - **Agentic Mode:** Multi-step refactoring, test generation, dependency updates
   - **Workspace Mode:** Architecture planning, specification-driven implementation

3. **Have Clear Success Metrics**
   - Functional correctness
   - Code quality (maintainability, readability)
   - Test coverage and reliability
   - Performance improvements
   - Security posture

4. **Mirror Real-World Scenarios**
   - Legacy system modernization
   - Distributed system design
   - Security vulnerability remediation
   - Performance optimization under constraints
   - Cross-team integration challenges

### What We Avoid

- âŒ Simple CRUD applications (too basic for AI advantage)
- âŒ Greenfield projects without constraints (not representative)
- âŒ Algorithm puzzles (better suited for traditional coding challenges)
- âŒ Challenges that can be solved with simple code generation

---

## ğŸ† The Three Challenges

Engineers must complete **2 out of 3 challenges**. Each challenge is designed to push different aspects of AI-augmented development.

### Challenge 1: Legacy System Modernization
**"From Monolith to Microservices"**

**Complexity Level:** High
**Estimated Time:** 6-8 hours
**Best AI Modes:** Agentic + Workspace

**Scenario:**
You inherit a legacy e-commerce monolith (Python/Django, ~15K LOC) with technical debt:
- Tightly coupled components
- No test coverage
- Mixed concerns (business logic in views)
- Database N+1 queries
- No API documentation

**Your Mission:**
1. Extract 3 bounded contexts into microservices
2. Implement API gateway with rate limiting
3. Add comprehensive test coverage (>80%)
4. Document APIs with OpenAPI specs
5. Maintain backward compatibility

**Why AI Excels Here:**
- Pattern recognition for identifying service boundaries
- Automated test generation across multiple services
- Refactoring large codebases while maintaining functionality
- API documentation generation from code

**Evaluation Criteria:**
- Architecture quality (service boundaries, coupling)
- Test coverage and quality
- API design (RESTful principles, documentation)
- Migration strategy (backward compatibility)
- Performance (query optimization)

---

### Challenge 2: Distributed System Design & Implementation
**"Build a Fault-Tolerant Event Processing Pipeline"**

**Complexity Level:** Very High
**Estimated Time:** 8-10 hours
**Best AI Modes:** Workspace + Chat

**Scenario:**
Design and implement a distributed event processing system that handles:
- 10,000+ events/second ingestion
- Multiple event types with different processing rules
- Guaranteed delivery with exactly-once semantics
- Dead letter queue handling
- Real-time monitoring and alerting

**Your Mission:**
1. Design system architecture (message broker, workers, storage)
2. Implement in language of choice (Go/Java/Rust recommended)
3. Add comprehensive error handling and retry logic
4. Implement observability (metrics, tracing, logging)
5. Write chaos engineering tests (network failures, node crashes)
6. Create deployment manifests (Docker + Kubernetes)

**Why AI Excels Here:**
- Architecture planning and specification development
- Boilerplate reduction for distributed patterns
- Test generation for failure scenarios
- Configuration file generation (K8s, Docker)
- Documentation of complex system interactions

**Evaluation Criteria:**
- System design (scalability, fault tolerance)
- Code quality (error handling, observability)
- Testing strategy (unit, integration, chaos)
- Deployment readiness (containerization, orchestration)
- Documentation (architecture diagrams, runbooks)

---

### Challenge 3: Security Hardening & Performance Optimization
**"Production-Ready in 24 Hours"**

**Complexity Level:** High
**Estimated Time:** 6-8 hours
**Best AI Modes:** Agentic + Chat

**Scenario:**
A startup's MVP Node.js/React application is going viral but has critical issues:
- 15 security vulnerabilities (OWASP Top 10)
- Page load times >5 seconds
- No authentication/authorization
- SQL injection risks
- Unoptimized database queries
- No rate limiting
- Memory leaks in production
- No security headers

**Your Mission:**
1. Fix all critical security vulnerabilities
2. Implement JWT-based authentication + RBAC
3. Optimize performance (target: <1s page load)
4. Add security headers and CSP
5. Implement rate limiting and DDoS protection
6. Add security testing (SAST, dependency scanning)
7. Create security documentation

**Why AI Excels Here:**
- Vulnerability scanning and fix suggestions
- Security best practices implementation
- Performance profiling and optimization
- Test generation for security scenarios
- Documentation of security measures

**Evaluation Criteria:**
- Security posture (vulnerability remediation)
- Authentication/authorization implementation
- Performance improvements (measurable metrics)
- Code quality (maintainability of security fixes)
- Testing (security tests, penetration testing setup)

---

## ğŸ“Š Evaluation Framework

### Scoring Rubric (Per Challenge)

Each challenge is scored out of 100 points across 5 dimensions:

| Dimension | Weight | Description |
|-----------|--------|-------------|
| **Functional Correctness** | 25% | Does it work? Meets requirements? |
| **Code Quality** | 25% | Maintainable, readable, follows best practices? |
| **AI Utilization** | 20% | Effective use of AI modes, prompt engineering? |
| **Testing & Reliability** | 15% | Test coverage, edge cases, error handling? |
| **Documentation** | 15% | Clear architecture, API docs, deployment guides? |

### Detailed Evaluation Criteria

#### 1. Functional Correctness (25 points)
- **Excellent (20-25):** All requirements met, edge cases handled, production-ready
- **Good (15-19):** Core requirements met, minor edge cases missed
- **Adequate (10-14):** Basic functionality works, some requirements incomplete
- **Poor (0-9):** Major functionality missing or broken

#### 2. Code Quality (25 points)
- **Excellent (20-25):** Clean architecture, SOLID principles, minimal technical debt
- **Good (15-19):** Well-structured, some refactoring opportunities exist
- **Adequate (10-14):** Functional but shows code smells, coupling issues
- **Poor (0-9):** Hard to maintain, tight coupling, anti-patterns

#### 3. AI Utilization (20 points)
- **Excellent (16-20):** Strategic use of multiple AI modes, custom prompts, context engineering
- **Good (12-15):** Effective use of AI for scaffolding, documentation, testing
- **Adequate (8-11):** Basic AI usage, mostly autocomplete and simple queries
- **Poor (0-7):** Minimal AI usage, missed opportunities for AI assistance

#### 4. Testing & Reliability (15 points)
- **Excellent (12-15):** >80% coverage, integration tests, edge cases, chaos/security tests
- **Good (9-11):** >60% coverage, good unit tests, some integration tests
- **Adequate (6-8):** Basic tests present, <60% coverage
- **Poor (0-5):** Minimal or no tests

#### 5. Documentation (15 points)
- **Excellent (12-15):** Architecture diagrams, API docs, deployment guides, decision log
- **Good (9-11):** Good README, API documentation, setup instructions
- **Adequate (6-8):** Basic README, minimal documentation
- **Poor (0-5):** Little to no documentation

### Overall Performance

| Total Score | Grade | Interpretation |
|-------------|-------|----------------|
| 90-100 | A+ | Exceptional AI-augmented engineering |
| 80-89 | A | Strong demonstration of AI capabilities |
| 70-79 | B | Good understanding of AI tools |
| 60-69 | C | Adequate but missed AI opportunities |
| <60 | D/F | Insufficient demonstration |

---

## ğŸ› ï¸ Understanding the Three AI Modes: A Deep Dive

The power of AI-augmented coding lies in knowing **which mode to use when**. Think of these modes as different tools in your workshopâ€”each designed for specific tasks.

---

### ğŸ—£ï¸ Chat Mode: Your Expert Colleague

**Metaphor:** Like asking a senior developer sitting next to you a quick question

**What It Is:**
- Interactive Q&A with an AI coding assistant
- You ask, AI answers
- No direct code modification
- Context limited to the conversation

**The Key Difference:**
- **You're in control:** AI suggests, you implement
- **Fast iteration:** Get answers in seconds
- **No file access:** AI can't modify your codebase directly
- **Copy-paste workflow:** You manually apply suggestions

**Perfect For:**

#### 1. Quick Documentation Lookups
```
You: "What's the syntax for Kubernetes liveness probes?"
AI: [Provides exact YAML syntax]
You: [Copy-paste into your manifest]
```

#### 2. Explaining Mysterious Code
```
You: [Paste 20 lines of confusing code]
     "What does this do?"
AI: "This implements the circuit breaker pattern..."
```

#### 3. Debugging Specific Errors
```
You: [Paste stack trace]
     "Why am I getting this error?"
AI: "The issue is on line 42â€”you're calling .map() on null.
     Add a null check before the map operation."
You: [Make the fix manually]
```

#### 4. API Exploration
```
You: "How do I use the Stripe API to create a payment intent?"
AI: [Provides example code with explanations]
You: [Adapt the example to your needs]
```

**Real-World Example:**

Imagine you're working on Challenge 3 (Security & Performance) and encounter this error:
```javascript
TypeError: Cannot read property 'user_id' from undefined
```

**Chat Mode Workflow:**
1. You paste the error + surrounding code
2. AI explains: "The req.session is undefined because you haven't initialized session middleware"
3. AI suggests: "Add express-session middleware before your routes"
4. You manually add the middleware
5. Total time: 2 minutes vs. 20 minutes reading docs

**When NOT to Use Chat:**
- âŒ Need to modify 10+ files (too much copy-paste)
- âŒ Large refactoring (manual work gets error-prone)
- âŒ Generating complete test suites (too tedious)

---

### ğŸ¤– Agentic Mode: Your Autonomous Developer

**Metaphor:** Like delegating a complete task to a junior developer who can work independently

**What It Is:**
- AI takes autonomous action on your codebase
- Can read, write, and modify multiple files
- Executes multi-step plans
- Runs tests and validates changes

**The Key Difference:**
- **AI drives:** You specify the goal, AI figures out how
- **Multi-step execution:** AI breaks down tasks and executes them
- **File system access:** AI can modify your entire codebase
- **Validation loops:** AI can run tests and iterate

**Perfect For:**

#### 1. Multi-File Refactoring
```
You: "Extract the authentication logic from all route files
      into a separate AuthService class"

AI: [Analyzes codebase]
    1. Creates src/services/AuthService.js
    2. Moves auth logic from 12 route files
    3. Updates all imports
    4. Runs tests to verify nothing broke
    5. Reports: "Refactoring complete. All tests passing."
```

**Without Agentic Mode:** You'd spend 2-3 hours manually tracking down every auth reference.
**With Agentic Mode:** 15 minutes, AI handles all the tedious coordination.

#### 2. Test Suite Generation
```
You: "Generate comprehensive unit tests for the UserService class.
      I need >80% coverage with edge cases."

AI: [Analyzes UserService.js]
    1. Reads the class methods
    2. Identifies edge cases
    3. Creates UserService.test.js with 25 test cases
    4. Runs tests to verify they work
    5. Generates coverage report
    6. Reports: "82% coverage achieved"
```

**Without Agentic Mode:** You'd write each test manually (3-4 hours).
**With Agentic Mode:** AI generates comprehensive tests in 20 minutes.

#### 3. Dependency Updates Across Codebase
```
You: "Update all React class components to functional components with hooks"

AI: [Scans codebase]
    1. Identifies 18 class components
    2. Converts each to functional + useState/useEffect
    3. Updates all prop types
    4. Fixes lifecycle method equivalents
    5. Runs tests after each conversion
    6. Reports: "All components converted. Tests passing."
```

**Real-World Example: Challenge 1 (Legacy Modernization)**

You need to extract the Product Catalog into a microservice:

**Agentic Mode Workflow:**
```
You: "Extract the products/ Django app into a standalone FastAPI microservice:
      1. Create new FastAPI project structure
      2. Port Django models to SQLAlchemy
      3. Convert Django views to FastAPI endpoints
      4. Generate OpenAPI documentation
      5. Create comprehensive tests
      6. Ensure backward-compatible API"

AI: [Works for 30 minutes]
    - Creates product-service/ directory
    - Generates 8 files (models, routes, schemas, tests, config)
    - Ports 3 Django models to SQLAlchemy
    - Creates 7 REST endpoints
    - Generates OpenAPI spec
    - Writes 15 test cases
    - Runs tests: All passing

You: [Review the code, make business logic adjustments]
     [Total time: 1 hour vs. 4-5 hours manually]
```

**The Power Move:**
Agentic mode shines when you have **clear requirements** but **tedious execution**. It's perfect for:
- "Do this pattern across 20 files"
- "Generate boilerplate for X"
- "Migrate from framework A to framework B"

**When NOT to Use Agentic:**
- âŒ Exploratory work (you don't know what you want yet)
- âŒ Novel business logic (AI needs specifications)
- âŒ High-level architecture decisions (needs human judgment)

---

### ğŸ—ï¸ Workspace Mode: Your Technical Architect

**Metaphor:** Like having a system architect create detailed specifications and blueprints before building

**What It Is:**
- Specification-first development
- AI helps plan before implementation
- Creates detailed technical designs
- Coordinates across multiple components

**The Key Difference:**
- **Planning over execution:** Focus on WHAT and WHY, not just HOW
- **Specification-driven:** Creates specs that other modes can execute
- **System-level thinking:** Considers entire architecture, not just code
- **Decision documentation:** Captures trade-offs and rationale

**Perfect For:**

#### 1. Architecture Planning
```
You: "I need to design a fault-tolerant event processing system that:
      - Handles 10K events/second
      - Guarantees exactly-once processing
      - Recovers from node failures
      - Runs on Kubernetes

      Provide a complete architecture with technology choices and trade-offs."

AI: [Creates comprehensive specification]

    ## System Architecture Specification

    ### Component Diagram
    [ASCII diagram of event flow]

    ### Technology Stack
    - Message Broker: Kafka (vs RabbitMQ)
      Rationale: Better throughput, built-in partitioning
      Trade-off: More complex operations

    - State Store: PostgreSQL (vs Cassandra)
      Rationale: ACID guarantees for exactly-once
      Trade-off: Lower write throughput

    - Worker Framework: Go + Sarama
      Rationale: Great concurrency, mature Kafka client
      Trade-off: Team needs Go experience

    ### Data Flow
    1. Events â†’ Load Balancer â†’ API Gateway
    2. Gateway â†’ Kafka Topic (partitioned by event.user_id)
    3. Consumer Group pulls from Kafka
    4. Workers process with transaction outbox pattern
    5. State committed to PostgreSQL + Kafka offset

    ### Fault Tolerance Strategy
    - Consumer group rebalancing for node failure
    - Transaction outbox for exactly-once semantics
    - Circuit breakers for downstream dependencies
    - Dead letter queue for poison pills

    ### Deployment Strategy
    - Kubernetes StatefulSet for workers
    - Horizontal Pod Autoscaler on queue lag
    - PodDisruptionBudget for graceful updates

    ### Implementation Phases
    Phase 1: Basic event ingestion (Week 1)
    Phase 2: Exactly-once processing (Week 2)
    Phase 3: Fault tolerance + chaos tests (Week 3)
    Phase 4: Observability + production readiness (Week 4)
```

**You:** [Now have a complete blueprint to execute with Agentic mode]

#### 2. Feature Specification
```
You: "I need to add JWT authentication to this Express API.
      The system needs:
      - Role-based access control (admin, user, guest)
      - Refresh token mechanism
      - Rate limiting on auth endpoints
      - Audit logging

      Create a detailed specification."

AI: [Generates specification document]

    ## Authentication System Specification

    ### Authentication Flow
    [Sequence diagram: Login â†’ Token â†’ Refresh â†’ Logout]

    ### API Endpoints
    POST /auth/login
      Request: { email, password }
      Response: { accessToken, refreshToken, user }
      Rate Limit: 5 req/min per IP

    POST /auth/refresh
      Request: { refreshToken }
      Response: { accessToken }
      Rate Limit: 10 req/min per user

    POST /auth/logout
      Request: { refreshToken }
      Response: { success: true }

    ### Token Structure
    Access Token:
      - Algorithm: RS256 (asymmetric)
      - Payload: { userId, role, permissions, exp }
      - Expiry: 15 minutes

    Refresh Token:
      - Stored in database (revocable)
      - Expiry: 7 days
      - One-time use (rotation)

    ### Authorization Middleware
    requireAuth(['admin']) â†’ Checks role
    requirePermission('users:write') â†’ Fine-grained

    ### Database Schema
    refresh_tokens table:
      - id (uuid, primary key)
      - user_id (foreign key)
      - token_hash (bcrypt)
      - expires_at (timestamp)
      - created_at (timestamp)

    auth_audit_log table:
      - id (uuid)
      - user_id
      - action ('login', 'logout', 'failed_login')
      - ip_address
      - user_agent
      - timestamp

    ### Security Considerations
    - Password: bcrypt with salt rounds = 12
    - Tokens: Signed with private key, verified with public
    - HTTPS only (secure cookie flags)
    - CSRF protection via double-submit cookie

    ### Testing Requirements
    - Unit tests: Token generation, validation, rotation
    - Integration tests: Full auth flows
    - Security tests: Brute force, token tampering
    - Load tests: Handle 1000 concurrent logins
```

**Now hand this spec to Agentic mode:** "Implement this specification"

#### 3. Migration Planning
```
You: "Plan a migration from this Django monolith to microservices.
      We need to:
      - Minimize downtime
      - Maintain backward compatibility
      - Migrate data safely
      - Keep team productive during migration"

AI: [Creates migration strategy]

    ## Microservices Migration Strategy

    ### Approach: Strangler Fig Pattern
    - Gradually extract services while monolith runs
    - API Gateway routes traffic to new services incrementally
    - Dual-write to old and new databases during transition

    ### Service Extraction Order
    1. Product Catalog (Week 1-2) - Lowest coupling
    2. User Service (Week 3-4) - Medium coupling
    3. Order Processing (Week 5-6) - Highest coupling

    ### Phase 1: Product Catalog
    Week 1:
      - Create FastAPI service skeleton
      - Implement read-only endpoints
      - Point API Gateway to new service
      - Monitor for issues
      - **Rollback strategy:** Flip gateway back to monolith

    Week 2:
      - Add write endpoints
      - Dual-write to both DBs
      - Validate data consistency
      - Switch reads to new DB
      - **Rollback strategy:** Revert DB reads to monolith

    ### Backward Compatibility Strategy
    - Legacy endpoints stay in monolith
    - New /v2 endpoints in microservices
    - API Gateway provides facade layer
    - Gradual client migration

    ### Data Migration
    - ETL pipeline: Monolith DB â†’ Microservice DB
    - Validation: Compare row counts, checksums
    - Dual-write period: 2 weeks per service
    - Cutover: Switch after validation passes

    ### Risk Mitigation
    - Feature flags for gradual rollout
    - Comprehensive integration tests
    - Load testing before cutover
    - Dedicated rollback procedures
```

**Real-World Example: Challenge 2 (Distributed Systems)**

You're facing a blank screen. Where do you even start?

**Workspace Mode Workflow:**
```
You: "Design a distributed event processing system for Challenge 2.
      Requirements: 10K events/sec, exactly-once, fault-tolerant.
      Provide complete architecture and implementation plan."

AI: [Generates 15-page specification]
    - System architecture diagram
    - Component breakdown
    - Technology justification
    - API contracts
    - Data schemas
    - Deployment strategy
    - Testing approach
    - Implementation phases

You: [Review the architecture]
     "Good, but I want to use Go instead of Java"

AI: [Updates specification for Go]
    - Adjusts library recommendations
    - Updates code examples
    - Modifies deployment approach

You: "Perfect! Now let's implement Phase 1"
     [Switch to Agentic mode with the spec]

Total planning time: 1 hour
Result: Clear blueprint that saves 4-5 hours of trial and error
```

**The Strategic Value:**
Workspace mode prevents you from:
- âŒ Starting to code without a plan
- âŒ Making technology choices you'll regret
- âŒ Missing critical requirements
- âŒ Building the wrong thing efficiently

**When NOT to Use Workspace:**
- âŒ Small, well-understood tasks (overkill)
- âŒ When you already have detailed specs
- âŒ Urgent bug fixes (use Chat instead)

---

## ğŸ¯ Mode Selection Decision Tree

**Use this flowchart to choose the right mode:**

```
START: What do you need to do?

â”œâ”€ "I have a quick question"
â”‚  â””â”€> CHAT MODE
â”‚     Examples: "How does this work?" "Why this error?" "What's the syntax?"
â”‚
â”œâ”€ "I need to modify code across multiple files"
â”‚  â””â”€> AGENTIC MODE
â”‚     Examples: Refactoring, test generation, pattern application
â”‚
â”œâ”€ "I need to design a system or plan an approach"
â”‚  â””â”€> WORKSPACE MODE
â”‚     Examples: Architecture, migration strategy, feature specification
â”‚
â””â”€ "I need to do all three!"
   â””â”€> COMBINE MODES
      1. Workspace: Create specification
      2. Agentic: Implement the spec
      3. Chat: Debug issues during implementation
```

---

## ğŸ”„ The Power of Combining Modes

**The most effective AI-augmented workflow uses all three modes strategically:**

### Example: Building a New Feature

**Scenario:** Add real-time notifications to your app

**Step 1: Workspace Mode (30 min)**
```
"Design a real-time notification system with:
- WebSocket connections
- Message queue for reliability
- Redis for presence
- PostgreSQL for persistence"

Output: Complete architecture spec
```

**Step 2: Agentic Mode (2-3 hours)**
```
"Implement the notification system based on this specification"
[Paste the Workspace spec]

AI: Creates 15 files, writes tests, generates docs
```

**Step 3: Chat Mode (as needed)**
```
Encounter WebSocket error â†’ Ask Chat
"Why are WebSocket connections dropping?"

AI: "You need to implement heartbeat pings..."
[Quick fix]
```

**Total Time:** 3-4 hours
**Without AI:** 10-12 hours
**Quality:** Better (comprehensive tests, good architecture)

---

## ğŸ’¡ Pro Tips for Mode Mastery

### Chat Mode Mastery
1. **Be specific with context:** Don't just paste codeâ€”explain what it should do
2. **Ask follow-up questions:** Drill deeper when the first answer isn't clear
3. **Request examples:** "Show me an example" gets better results than theory

### Agentic Mode Mastery
1. **Write clear specifications:** The better your requirements, the better the output
2. **Request validation:** "Run tests after each change"
3. **Review carefully:** AI makes mistakesâ€”always review multi-file changes

### Workspace Mode Mastery
1. **Start broad, then refine:** Get the high-level design first, then drill into details
2. **Ask for trade-offs:** "Compare approach A vs B" reveals important considerations
3. **Iterate on the plan:** Refining a spec is faster than refactoring code

---

## ğŸ“Š Mode Effectiveness by Challenge

| Challenge | Workspace | Agentic | Chat | Why |
|-----------|-----------|---------|------|-----|
| **Challenge 1: Legacy Modernization** | 30% | 60% | 10% | Lots of refactoring (Agentic), planning (Workspace), debugging (Chat) |
| **Challenge 2: Distributed Systems** | 40% | 30% | 30% | Complex design needed (Workspace), implementation (Agentic), learning curve (Chat) |
| **Challenge 3: Security & Performance** | 15% | 50% | 35% | Security fixes (Agentic), debugging (Chat), light planning (Workspace) |

---

## ğŸŒŸ Real-World Success Pattern

**Engineers who score 90+ consistently use this pattern:**

1. **Morning:** Use Workspace mode to plan the day's work
   - Create specifications for what to build
   - Document architectural decisions
   - Break down large tasks

2. **Midday:** Use Agentic mode for heavy lifting
   - Implement specs from morning
   - Generate tests
   - Refactor legacy code

3. **Afternoon:** Use Chat mode for debugging and refinement
   - Fix issues found during testing
   - Optimize performance
   - Learn new APIs

4. **End of day:** Use Workspace mode to document
   - Create architecture decision records
   - Update implementation plans
   - Prepare specifications for tomorrow

**Result:** Maximum productivity with high quality

---

### Alternative/Complementary Tools

- **Claude Code:** Terminal-based agentic workflows (like Agentic mode)
- **Cursor:** Deep codebase understanding (combines all modes)
- **Cline:** Autonomous task execution (like Agentic mode)
- **v0 by Vercel:** UI component generation (specialized Agentic)
- **AWS Kiro:** Specification-driven workflow (like Workspace mode)

### Prompt Engineering Tips

1. **Be Specific:** "Extract user authentication into a separate microservice with JWT tokens" vs "make it better"
2. **Provide Context:** Share architecture constraints, tech stack, requirements
3. **Iterate:** Start with planning prompts, then implementation, then refinement
4. **Use Examples:** Show desired patterns for AI to follow
5. **Combine Modes:** Use Workspace for planning, Agentic for execution, Chat for debugging

---

## ğŸ“ Repository Structure

```
ai-coding-challenges/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ PROBLEM-STATEMENT.md               # Detailed problem formulation
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ ai-coding-effectiveness.md     # Research on AI coding patterns
â”‚   â”œâ”€â”€ challenge-design-principles.md # How we designed these challenges
â”‚   â””â”€â”€ evaluation-methodology.md      # Scoring approach and validation
â”œâ”€â”€ challenges/
â”‚   â”œâ”€â”€ challenge-1-legacy-modernization/
â”‚   â”‚   â”œâ”€â”€ README.md                  # Challenge details
â”‚   â”‚   â”œâ”€â”€ starter-code/              # Legacy monolith codebase
â”‚   â”‚   â”œâ”€â”€ requirements.md            # Detailed requirements
â”‚   â”‚   â”œâ”€â”€ evaluation-rubric.md       # Specific scoring criteria
â”‚   â”‚   â””â”€â”€ sample-solution/           # Reference implementation
â”‚   â”œâ”€â”€ challenge-2-distributed-systems/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ requirements.md
â”‚   â”‚   â”œâ”€â”€ architecture-constraints.md
â”‚   â”‚   â”œâ”€â”€ evaluation-rubric.md
â”‚   â”‚   â””â”€â”€ sample-solution/
â”‚   â””â”€â”€ challenge-3-security-performance/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ starter-code/              # Vulnerable application
â”‚       â”œâ”€â”€ requirements.md
â”‚       â”œâ”€â”€ evaluation-rubric.md
â”‚       â””â”€â”€ sample-solution/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ submission-template.md         # How to submit solutions
â”‚   â”œâ”€â”€ architecture-doc-template.md   # For documenting design decisions
â”‚   â””â”€â”€ ai-usage-log-template.md       # Track AI interactions
â””â”€â”€ evaluation/
    â”œâ”€â”€ scoring-guide.md               # For evaluators
    â”œâ”€â”€ automated-tests/               # Validation scripts
    â””â”€â”€ benchmarks/                    # Performance baselines
```

---

## ğŸš€ Getting Started

### For Challenge Participants

1. **Choose Your Challenges:** Select 2 out of 3 challenges based on your interests
2. **Set Up Environment:** Clone starter code, review requirements
3. **Plan Your Approach:** Use AI Workspace mode to create specifications
4. **Execute:** Leverage different AI modes strategically
5. **Document:** Track AI usage, architectural decisions
6. **Submit:** Follow submission template with code + documentation

### For Challenge Designers

1. **Review Research:** Read `research/` folder for design principles
2. **Understand Evaluation:** Study scoring rubrics and methodology
3. **Create Variations:** Use templates to design new challenges
4. **Validate:** Test with real engineers and AI tools

---

## ğŸ“ Learning Objectives

By completing these challenges, engineers will:

1. **Master AI Mode Selection**
   - Understand when to use chat vs agentic vs workspace modes
   - Develop prompt engineering skills
   - Learn context engineering for better AI outputs

2. **Improve System Design**
   - Architecture planning with AI assistance
   - Trade-off analysis and decision documentation
   - Specification-driven development

3. **Enhance Code Quality**
   - AI-assisted refactoring strategies
   - Test generation and coverage improvement
   - Security best practices implementation

4. **Build Real-World Skills**
   - Legacy system modernization
   - Distributed system design
   - Production readiness (security, performance, observability)

---

## ğŸ“ˆ Success Metrics

### For Participants

- **Completion Rate:** Successfully complete 2/3 challenges
- **Score Target:** Achieve 80+ average score across completed challenges
- **Time Efficiency:** Complete within estimated time windows
- **AI Effectiveness:** Demonstrate strategic use of multiple AI modes

### For Program

- **Differentiation:** Challenges should show clear AI advantage (30%+ faster than non-AI)
- **Quality:** Solutions should meet production standards
- **Learning:** Participants report improved AI tool proficiency
- **Realism:** Challenges mirror actual engineering work

---

## ğŸ” Research Foundation

### Key Questions We're Exploring

1. **Which types of engineering tasks benefit most from AI augmentation?**
2. **How do different AI modes (chat, agentic, workspace) complement each other?**
3. **What prompt engineering patterns yield highest quality outputs?**
4. **Can AI-augmented development maintain quality while increasing speed?**
5. **What skills do engineers need to maximize AI tool effectiveness?**

### Research Areas

- **AI Coding Patterns:** Common workflows and anti-patterns
- **Mode Selection:** Decision framework for AI mode usage
- **Quality Metrics:** Measuring AI-generated code quality
- **Learning Curves:** Time to proficiency with AI tools
- **Enterprise Adoption:** Barriers and success factors

**Full Research:** See `research/` directory

---

## ğŸ¤ Contributing

This is a living framework. We welcome:

- **New Challenge Designs:** Following our design principles
- **Improved Starter Code:** More realistic legacy codebases
- **Enhanced Rubrics:** Better evaluation criteria
- **Research Findings:** Data on AI coding effectiveness
- **Tool Integrations:** Support for new AI coding platforms

---

## ğŸ“š Additional Resources

### AI Coding Best Practices
- [GitHub Copilot Documentation](https://docs.github.com/copilot)
- [Anthropic Claude Code Guide](https://docs.anthropic.com/claude-code)
- [Cursor Tips & Tricks](https://cursor.sh/docs)

### System Design Resources
- [System Design Primer](https://github.com/donnemartin/system-design-primer)
- [Microservices Patterns](https://microservices.io/patterns/)
- [Distributed Systems Resources](https://github.com/theanalyst/awesome-distributed-systems)

### Security Resources
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [Security Testing Guide](https://owasp.org/www-project-web-security-testing-guide/)

---

## ğŸ“„ License & Usage

**Open Source:** MIT License
**Attribution:** AI Coding Challenge Framework
**Commercial Use:** Permitted with attribution

---

## ğŸ“ Contact & Support

- **Issues:** GitHub Issues for bug reports
- **Discussions:** GitHub Discussions for questions
- **Updates:** Watch repository for new challenges

---

**Created:** 2025-11-19
**Version:** 1.0
**Status:** Initial Framework

---

*Push the limits of AI-augmented coding. Build the future.*
